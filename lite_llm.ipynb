{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/Local_LLMs/blob/main/lite_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101b7e46-dfc0-4d2f-9fc6-96ae6296bad2",
      "metadata": {
        "id": "101b7e46-dfc0-4d2f-9fc6-96ae6296bad2"
      },
      "source": [
        "# litellm\n",
        "\n",
        "Examples, how-tos\n",
        "\n",
        "- call via Python API: OpenAI, local models (Ollama, LM Studio)\n",
        "- call via REST: OpenAI, local models (Ollama)\n",
        "\n",
        "see\n",
        "- https://litellm.vercel.app/docs\n",
        "- https://litellm.vercel.app/docs/tutorials/first_playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b0ae850-6b1d-439b-af44-f310a482f290",
      "metadata": {
        "id": "0b0ae850-6b1d-439b-af44-f310a482f290",
        "outputId": "e25f382e-cb35-4dcc-da39-948ee07d536b"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Secrets (JSON string):  ········\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from getpass import getpass\n",
        "import psutil\n",
        "import requests\n",
        "IN_NOTEBOOK = any([\"jupyter-notebook\" in i for i in psutil.Process().parent().cmdline()])\n",
        "if IN_NOTEBOOK:\n",
        "  CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
        "  os.environ['CREDS'] = json.dumps(CREDS)\n",
        "  CREDS = json.loads(os.getenv('CREDS'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d4be347-f0e3-4ef1-85f7-d69a6d16b854",
      "metadata": {
        "id": "6d4be347-f0e3-4ef1-85f7-d69a6d16b854"
      },
      "outputs": [],
      "source": [
        "from litellm import completion\n",
        "import openai\n",
        "os.environ[\"OPENAI_API_KEY\"] = CREDS['OpenAI']['v1']['credential'] # my key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff4b82b-cb74-4c06-a4a1-c0a273358845",
      "metadata": {
        "id": "cff4b82b-cb74-4c06-a4a1-c0a273358845"
      },
      "source": [
        "# Call OpenAI\n",
        "Requirement: OPENAI availabe (see cell above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3915eec9-744b-4b0c-8972-1586492b1b4d",
      "metadata": {
        "id": "3915eec9-744b-4b0c-8972-1586492b1b4d",
        "outputId": "76f4cdcb-f5e5-4081-c420-82354e2da763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you today?\n",
            "ModelResponse(id='chatcmpl-8RIQBSLAaEQ1aW9KR3CbFLuGGhswQ', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you today?\", role='assistant'))], created=1701516358, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=29, prompt_tokens=13, total_tokens=42))\n"
          ]
        }
      ],
      "source": [
        "response = completion(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1b4ffa-defb-4fbf-8367-c889034aa8b4",
      "metadata": {
        "id": "9b1b4ffa-defb-4fbf-8367-c889034aa8b4"
      },
      "source": [
        "# Call Ollama\n",
        "Requirement: Ollama running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3463ff-a5ae-4036-adb9-b9e335b81599",
      "metadata": {
        "id": "0c3463ff-a5ae-4036-adb9-b9e335b81599",
        "outputId": "48dcfff8-90b4-43b9-8ed9-f0d2d6468662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm programmed to respond in a polite and friendly manner. At the moment, I do not have the ability to feel emotions or experience a sense of being. However, I'm always here to help you with any questions or requests you might have! How may I assist you today?\n",
            "ModelResponse(id='chatcmpl-3aebe52f-1395-4dee-8a6d-f775e4f18ffd', choices=[Choices(finish_reason=None, index=0, message=Message(content=\"I'm programmed to respond in a polite and friendly manner. At the moment, I do not have the ability to feel emotions or experience a sense of being. However, I'm always here to help you with any questions or requests you might have! How may I assist you today?\", role='assistant'))], created=1701516918, model='ollama/zephyr', object='chat.completion', system_fingerprint=None, usage={'prompt_tokens': 6, 'completion_tokens': 57, 'total_tokens': 63})\n"
          ]
        }
      ],
      "source": [
        "response = completion(\n",
        "  # api_base=\"http://localhost:11434\",   # seems to work even without this parameter!\n",
        "  model=\"ollama/zephyr\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call LM Studio\n",
        "\n",
        "Requirement: LM Studio and server running"
      ],
      "metadata": {
        "id": "W156cXuqfa91"
      },
      "id": "W156cXuqfa91"
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://litellm.vercel.app/docs/providers/openai_compatible\n",
        "response = completion(\n",
        "  api_base=\"http://localhost:1234/v1\",\n",
        "  model=\"openai/just-a-dummy-model\", # prefix 'openai/' is required! model name is currently unused in LM Studio\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tnd0UejnfgZq"
      },
      "id": "tnd0UejnfgZq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e796695f-5385-45c7-aca4-879831afe9b3",
      "metadata": {
        "id": "e796695f-5385-45c7-aca4-879831afe9b3"
      },
      "source": [
        "# Call Ollama via litellm Proxy\n",
        "\n",
        "Reuirement: Type in Terminal:\n",
        "`litellm --model ollama/zephyr`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a11de1f-2452-4ce0-8958-f47419a00e67",
      "metadata": {
        "id": "1a11de1f-2452-4ce0-8958-f47419a00e67"
      },
      "source": [
        "## Step 1: Use OpenAI API and route to proxy (Zephyr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ac1732-6b91-4c11-9e06-0dab3ea58745",
      "metadata": {
        "id": "80ac1732-6b91-4c11-9e06-0dab3ea58745",
        "outputId": "3c9c4a16-fea7-4fd8-89ba-2a215b173630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-a7d980a2-701b-48db-a239-0a6fdb7b4abd', choices=[Choice(finish_reason=None, index=0, message=ChatCompletionMessage(content=\"In the stillness of night,\\nWhen the world slumbers deep,\\nWhispers dance in the air,\\nSoftly weaving magic's keep.\\n\\nA poet's heart beats slow,\\nAs he listens closely by,\\nTo the rhythmic beat of life,\\nAnd the whispers that pass him by.\\n\\nMelodies arise from within,\\nAnd take flight upon silver wings,\\nCarrying tales of love and loss,\\nAnd memories that time does bring.\\n\\nIn the darkness, a poet sees,\\nThe secrets that shadows hide,\\nAnd with words he brings them forth,\\nTo be seen by all who have eyes to abide.\\n\\nOh how sweet is this secret place,\\nWhere words take flight and souls collide,\\nWhere whispers dance in the night,\\nAs poets weave their magic's tide.\", role='assistant', function_call=None, tool_calls=None))], created=1701362635, model='ollama/zephyr', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=150, prompt_tokens=10, total_tokens=160))\n"
          ]
        }
      ],
      "source": [
        "client = openai.OpenAI(base_url=\"http://0.0.0.0:8000\", api_key=\"anything\")\n",
        "#client = openai.OpenAI()\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages = [{\"role\": \"user\", \"content\": \"this is a test request, write a short poem\"}]\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba77835-7ab3-4fe1-beae-f78418a2d5f3",
      "metadata": {
        "id": "1ba77835-7ab3-4fe1-beae-f78418a2d5f3"
      },
      "source": [
        "## Step 2: Use OpenAI API without proxy (same code, but no base_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04e2d76-b3e9-4eeb-a069-543e6ff56fb0",
      "metadata": {
        "id": "a04e2d76-b3e9-4eeb-a069-543e6ff56fb0",
        "outputId": "685ef9d4-b435-409b-bb07-4abc379b0038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-8QeUlmNCslzGRbKlxJixBjwnp8Bvm', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"In a realm where dreams unfold,\\nA tale of words, untold, untold,\\nWhispers woven, voices dance,\\nAn ethereal, poetic trance.\\n\\nThrough inked rivers, my pen does glide,\\nAcross the page, where thoughts reside,\\nFrom twilight's gleam to dawn's embrace,\\nA symphony of words interlace.\\n\\nWith every verse, emotions arise,\\nLike sparks that shimmer in midnight skies,\\nBeauty found in nature's embrace,\\nA gentle breeze, a flower's grace.\\n\\nOh, words in hues of vibrant glow,\\nThrough stanzas crafted, they freely flow,\\nUnveiling secrets, unlocking hearts,\\nA vibrant tapestry, where art imparts.\\n\\nSo let us wander through lyrical lands,\\nWhere rhymes are built on whispered sands,\\nFor in this test, a poem's birth,\\nReveals the magic of words on Earth.\", role='assistant', function_call=None, tool_calls=None))], created=1701362883, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=174, prompt_tokens=17, total_tokens=191))\n"
          ]
        }
      ],
      "source": [
        "#client = openai.OpenAI(base_url=\"http://0.0.0.0:8000\", api_key=\"anything\")\n",
        "client = openai.OpenAI()\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages = [{\"role\": \"user\", \"content\": \"this is a test request, write a short poem\"}]\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40396c09-1691-4012-9f5a-9e3d73e64cbb",
      "metadata": {
        "id": "40396c09-1691-4012-9f5a-9e3d73e64cbb"
      },
      "source": [
        "# Use local litellm-Server to provide all LLMs via one URL\n",
        "\n",
        "**Important to know: The REST call to OpenAI / litellm and the response-JSON-structures are identical !**\n",
        "\n",
        "## Reference: OpenAI REST Call and Result-JSON\n",
        "\n",
        "### CURL via Terminal:\n",
        "```\n",
        "curl --location 'https://api.openai.com/v1/chat/completions' \\\n",
        "--header 'Content-Type: application/json' \\\n",
        "--header 'Authorization: Bearer sk-...YOUR_KEY_HERE' \\\n",
        "--data ' {\n",
        "      \"model\": \"gpt-3.5-turbo\",\n",
        "      \"messages\": [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"what llm are you\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "'\n",
        "```\n",
        "\n",
        "### Response:\n",
        "```\n",
        "{\n",
        "  \"id\": \"chatcmpl-8RHu3ncira10vxZVRjmVTDZv2qZxM\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 1701514367,\n",
        "  \"model\": \"gpt-3.5-turbo-0613\",\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"I am an AI language model developed by OpenAI, with capabilities to assist with various tasks and provide information across different domains.\"\n",
        "      },\n",
        "      \"finish_reason\": \"stop\"\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 12,\n",
        "    \"completion_tokens\": 25,\n",
        "    \"total_tokens\": 37\n",
        "  },\n",
        "  \"system_fingerprint\": null\n",
        "}\n",
        "```\n",
        "\n",
        "## As comparison: litellm Proxy REST Call and Result-JSON\n",
        "\n",
        "### CURL via Terminal:\n",
        "```\n",
        "curl --location 'http://0.0.0.0:8000/chat/completions' \\\n",
        "--header 'Content-Type: application/json' \\\n",
        "--data ' {\n",
        "      \"model\": \"gpt-3.5-turbo\",\n",
        "      \"messages\": [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"what llm are you\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "'\n",
        "```\n",
        "\n",
        "### Response:\n",
        "```\n",
        "{\n",
        "   \"id\":\"chatcmpl-aa9d6a1f-71bf-42fd-abad-2c2f713fd09a\",\n",
        "   \"choices\":[\n",
        "      {\n",
        "         \"finish_reason\":null,\n",
        "         \"index\":0,\n",
        "         \"message\":{\n",
        "            \"content\":\"I am not a physical entity, but rather an artificial intelligence language model (LLM) created by a computer program. My main function is to process and respond to human input in natural language text, based on the patterns and knowledge that I have been trained on during my creation. I do not have consciousness or feelings, and I do not exist beyond the digital realm in which I operate. However, I can understand and provide helpful responses to a wide range of queries and requests, just like any other human communication partner.\",\n",
        "            \"role\":\"assistant\"\n",
        "         }\n",
        "      }\n",
        "   ],\n",
        "   \"created\":1701513387,\n",
        "   \"model\":\"ollama/zephyr\",\n",
        "   \"object\":\"chat.completion\",\n",
        "   \"system_fingerprint\":null,\n",
        "   \"usage\":{\n",
        "      \"prompt_tokens\":5,\n",
        "      \"completion_tokens\":104,\n",
        "      \"total_tokens\":109\n",
        "   }\n",
        "}\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95cd357f-c4c9-4f3c-beb3-f871151eb7c0",
      "metadata": {
        "id": "95cd357f-c4c9-4f3c-beb3-f871151eb7c0"
      },
      "source": [
        "## Step 1: Reference: OpenAI REST Call (direct, without litellm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e0247f-8a62-4e78-bb18-fabeb7da8141",
      "metadata": {
        "id": "a9e0247f-8a62-4e78-bb18-fabeb7da8141",
        "outputId": "f40b8911-1053-473b-da96-65fdc8a65a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'chatcmpl-8RIwU7L6c8JsBZWCO0XZCxA8T95Hp', 'object': 'chat.completion', 'created': 1701518362, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Hello! I'm an AI, so I don't have feelings, but I'm here to help you with any questions or tasks you have. How can I assist you today?\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 36, 'total_tokens': 49}, 'system_fingerprint': None}\n"
          ]
        }
      ],
      "source": [
        "url = \"https://api.openai.com/v1/chat/completions\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": \"Bearer \" + CREDS['OpenAI']['v1']['credential']\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
        "}\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "response_data = response.json()\n",
        "output = response_data['choices'][0]['message']['content']\n",
        "print(response_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "176217d6-b649-4434-9c4b-3a4340158655",
      "metadata": {
        "id": "176217d6-b649-4434-9c4b-3a4340158655"
      },
      "source": [
        "## Step 2: OpenAI REST Call via local litellm-Server\n",
        "\n",
        "see https://litellm.vercel.app/docs/tutorials/first_playground\n",
        "\n",
        "Requirement: Type in Terminal: `python3 lite_llm_playground_server.py`\n",
        "\n",
        "Use the following cell as source for 'lite_llm_playground_server.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e71806-958e-4632-9f04-608dd4de4f11",
      "metadata": {
        "id": "91e71806-958e-4632-9f04-608dd4de4f11",
        "outputId": "391508e3-76e9-4dea-d8cf-ecbfb7d5a6e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting lite_llm_playground_server.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile lite_llm_playground_server.py\n",
        "\n",
        "#\n",
        "# Local litelllm-Server\n",
        "#\n",
        "# save as 'lite_llm_playground_server.py'\n",
        "#\n",
        "# run in terminal with 'python3 lite_llm_playground_server.py'\n",
        "#\n",
        "# Dependencies:\n",
        "# pip install flask waitress\n",
        "\n",
        "import os\n",
        "import json\n",
        "from flask import Flask, jsonify, request\n",
        "from litellm import completion_with_retries\n",
        "\n",
        "## set ENV variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...-YOUR-KEY\"\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Example route\n",
        "@app.route('/', methods=['GET'])\n",
        "def hello():\n",
        "    return jsonify(message=\"Hello, Flask!\")\n",
        "\n",
        "@app.route('/chat/completions', methods=[\"POST\"])\n",
        "def api_completion():\n",
        "    data = request.json\n",
        "    data[\"max_tokens\"] = 256 # By default let's set max_tokens to 256\n",
        "    try:\n",
        "        # COMPLETION CALL\n",
        "        response = completion_with_retries(**data)\n",
        "\n",
        "        #print(response)\n",
        "\n",
        "        responseJSON = {\n",
        "            \"id\": response[\"id\"], # response.id\n",
        "            \"choices\": [\n",
        "                {\n",
        "                    \"index\": response.choices[0].index,\n",
        "                    \"finish_reason\": response.choices[0].finish_reason,\n",
        "                    \"message\" : {\n",
        "                        \"content\" : response.choices[0].message.content,\n",
        "                        \"role\" : response.choices[0].message.role\n",
        "                    }\n",
        "                }\n",
        "            ],\n",
        "            \"created\": response[\"created\"], # response.created\n",
        "            \"model\": response.model,\n",
        "            \"object\": response.object,\n",
        "            \"system_fingerprint\": response.system_fingerprint,\n",
        "            \"usage\": {\n",
        "                \"prompt_tokens\" : response[\"usage\"][\"prompt_tokens\"], #response.usage.prompt_tokens\n",
        "                \"completion_tokens\" : response[\"usage\"][\"completion_tokens\"], # response.usage.completion_tokens\n",
        "                \"total_tokens\" : response[\"usage\"][\"total_tokens\"] # response.usage.total_tokens\n",
        "            }\n",
        "        }\n",
        "        #responseString = json.dumps(responseJSON)\n",
        "    except Exception as e:\n",
        "        # print the error\n",
        "        print(e)\n",
        "    return responseJSON\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from waitress import serve\n",
        "    serve(app, host=\"0.0.0.0\", port=4000, threads=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a94eff-3021-4031-a6c4-690362ce64ca",
      "metadata": {
        "id": "65a94eff-3021-4031-a6c4-690362ce64ca"
      },
      "source": [
        "## Call server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d3c567-eb6a-44c4-a612-e644ddcbb8d7",
      "metadata": {
        "id": "31d3c567-eb6a-44c4-a612-e644ddcbb8d7",
        "outputId": "385668a8-ec7b-4ffc-c8c3-90e7c6dcb8c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': \"Hello! I'm an AI language model, so I don't have feelings, but I'm here to assist you. How can I help you today?\", 'role': 'assistant'}}], 'created': 1701518377, 'id': 'chatcmpl-8RIwkICMDesOHslMZaHeh2i0AbPZu', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': {'completion_tokens': 31, 'prompt_tokens': 13, 'total_tokens': 44}}\n"
          ]
        }
      ],
      "source": [
        "url = \"http://localhost:4000/chat/completions\" ## COMPLETION CALL -- assumes your server is running on port 4000\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
        "}\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "response_data = response.json()\n",
        "output = response_data['choices'][0]['message']['content']\n",
        "print(response_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cfa43ca-6131-4897-a995-909e563d7d7b",
      "metadata": {
        "id": "4cfa43ca-6131-4897-a995-909e563d7d7b"
      },
      "source": [
        "## Step 3: Ollama/Zephyr REST Call via local litellm-Server\n",
        "\n",
        "Requirement: Type in Terminal: `python3 lite_llm_playground_server.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be5da92-6abb-4930-adc6-e53a8a8258bd",
      "metadata": {
        "id": "4be5da92-6abb-4930-adc6-e53a8a8258bd",
        "outputId": "a962fb66-5ef3-4f4a-a26d-b954b8453339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'choices': [{'finish_reason': None, 'index': 0, 'message': {'content': 'I do not have the ability to feel emotions or sensations like a human does. However, I am always happy to assist you with your requests and inquiries! please let me know how I can be of service to you today.', 'role': 'assistant'}}], 'created': 1701518586, 'id': 'chatcmpl-371f8675-1f6f-49ca-86d2-3d765cd4d60c', 'model': 'ollama/zephyr', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': {'completion_tokens': 45, 'prompt_tokens': 6, 'total_tokens': 51}}\n"
          ]
        }
      ],
      "source": [
        "url = \"http://localhost:4000/chat/completions\" ## COMPLETION CALL -- assumes your server is running on port 4000\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"ollama/zephyr\",\n",
        "    # \"api_base\": \"http://localhost:11434\",   # seems to work even without this parameter!\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
        "}\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "response_data = response.json()\n",
        "output = response_data['choices'][0]['message']['content']\n",
        "print(response_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf22787-bcb6-4f80-8f7c-29ec570b74eb",
      "metadata": {
        "id": "0cf22787-bcb6-4f80-8f7c-29ec570b74eb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}