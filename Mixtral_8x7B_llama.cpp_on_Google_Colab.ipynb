{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/Local_LLMs/blob/main/Mixtral_8x7B_llama.cpp_on_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Approach: Mixtral-8x7B-Instruct via llama.cpp-python\n",
        "\n",
        "Google T4:\n",
        "- mixtral-8x7b: 4 token / sec\n",
        "- mistral-7b: 12 token /sec\n",
        "\n",
        "Source:\n",
        "https://koji-kanao.medium.com/run-mistral-7b-on-google-colab-less-than-20-lines-883c483df998"
      ],
      "metadata": {
        "id": "qNPPvNJXgPP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = 'mixtral-8x7b-instruct-v0.1.Q2_K'\n",
        "#model = 'openhermes-2-mistral-7b.Q5_K_M'\n",
        "\n",
        "if model == 'mixtral-8x7b-instruct-v0.1.Q2_K':\n",
        "\t!wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\n",
        "elif model == 'openhermes-2-mistral-7b.Q5_K_M':\n",
        "\t!wget https://huggingface.co/TheBloke/OpenHermes-2-Mistral-7B-GGUF/resolve/main/openhermes-2-mistral-7b.Q5_K_M.gguf\n",
        "elif model == 'dolphin-2.6-mixtral-8x7b.Q2_K':\n",
        "\t!wget https://huggingface.co/TheBloke/dolphin-2.6-mixtral-8x7b-GGUF/resolve/main/dolphin-2.6-mixtral-8x7b.Q2_K.gguf\n",
        "elif model == 'mistral-7b-instruct-v0.2.Q8':\n",
        "\t!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf\n",
        "elif model == 'dolphin-2.2.1-mistral-7b.Q8':\n",
        "\t!wget https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q8_0.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMjpE7rsjXCb",
        "outputId": "bc99fece-d384-42b4-bf3a-aa1e1ba306fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-04 13:24:32--  https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.24, 18.172.134.124, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/ac/ba/acba0635d39a127379c2c6ae1cefacc586bf413e8b044c5ca82daade27d7d503/d54b4f4ec06dbae558d25b2d1542417cdf9547907342db85eecd05b6e96e88f8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mixtral-8x7b-instruct-v0.1.Q2_K.gguf%3B+filename%3D%22mixtral-8x7b-instruct-v0.1.Q2_K.gguf%22%3B&Expires=1704633872&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNDYzMzg3Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2FjL2JhL2FjYmEwNjM1ZDM5YTEyNzM3OWMyYzZhZTFjZWZhY2M1ODZiZjQxM2U4YjA0NGM1Y2E4MmRhYWRlMjdkN2Q1MDMvZDU0YjRmNGVjMDZkYmFlNTU4ZDI1YjJkMTU0MjQxN2NkZjk1NDc5MDczNDJkYjg1ZWVjZDA1YjZlOTZlODhmOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Z%7E43v81aVE3tjP5gyLXkS6gWpzQiFjbB5ipUs63HZ%7EThFXzRd9iYQUmCSVixzvbNQVA1XuTGMIETeEBCaYxjo3MBNMAG3875Og-tWh3jFEYstB8z2BD%7EVNz%7EA%7Ej4Owb3mKiSSvynNc5CMDRcuinjEstWytNaRsF35rVhJjRV0bErqjpfUfsWGA3-mezAHM3%7EUMKpNayLZoEIGfgSQQcqoAq-CkkLkmmN1XxsKD8ac1JH9Ad0Hv6pkKMz2t5sMIXFEY5tdV7-mzNAanC-CkE6fMgK7A1fyvFajYQufv%7EgA-sleLLViMxQGUy8uSi%7EDX6GXsDTwneH8Yk%7EchivvZzRyQ__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2024-01-04 13:24:32--  https://cdn-lfs-us-1.huggingface.co/repos/ac/ba/acba0635d39a127379c2c6ae1cefacc586bf413e8b044c5ca82daade27d7d503/d54b4f4ec06dbae558d25b2d1542417cdf9547907342db85eecd05b6e96e88f8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mixtral-8x7b-instruct-v0.1.Q2_K.gguf%3B+filename%3D%22mixtral-8x7b-instruct-v0.1.Q2_K.gguf%22%3B&Expires=1704633872&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNDYzMzg3Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2FjL2JhL2FjYmEwNjM1ZDM5YTEyNzM3OWMyYzZhZTFjZWZhY2M1ODZiZjQxM2U4YjA0NGM1Y2E4MmRhYWRlMjdkN2Q1MDMvZDU0YjRmNGVjMDZkYmFlNTU4ZDI1YjJkMTU0MjQxN2NkZjk1NDc5MDczNDJkYjg1ZWVjZDA1YjZlOTZlODhmOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Z%7E43v81aVE3tjP5gyLXkS6gWpzQiFjbB5ipUs63HZ%7EThFXzRd9iYQUmCSVixzvbNQVA1XuTGMIETeEBCaYxjo3MBNMAG3875Og-tWh3jFEYstB8z2BD%7EVNz%7EA%7Ej4Owb3mKiSSvynNc5CMDRcuinjEstWytNaRsF35rVhJjRV0bErqjpfUfsWGA3-mezAHM3%7EUMKpNayLZoEIGfgSQQcqoAq-CkkLkmmN1XxsKD8ac1JH9Ad0Hv6pkKMz2t5sMIXFEY5tdV7-mzNAanC-CkE6fMgK7A1fyvFajYQufv%7EgA-sleLLViMxQGUy8uSi%7EDX6GXsDTwneH8Yk%7EchivvZzRyQ__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.156.107.80, 108.156.107.44, 108.156.107.29, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.156.107.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15644035008 (15G) [binary/octet-stream]\n",
            "Saving to: ‘mixtral-8x7b-instruct-v0.1.Q2_K.gguf’\n",
            "\n",
            "mixtral-8x7b-instru 100%[===================>]  14.57G  58.4MB/s    in 86s     \n",
            "\n",
            "2024-01-04 13:25:59 (173 MB/s) - ‘mixtral-8x7b-instruct-v0.1.Q2_K.gguf’ saved [15644035008/15644035008]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE9heykddUQb",
        "outputId": "551c69d9-a120-4474-c402-f8c0ee755867"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.26.tar.gz (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.26-cp310-cp310-manylinux_2_35_x86_64.whl size=8129857 sha256=aba41645e7b25039d3149a7b64e296e3bc43bb89bb194ffdababff396d2ca207\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/80/ce/ac6afea8c1d6fbcec7e14183033a5b2796c742d4f470010c72\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import ctypes\n",
        "import json\n",
        "\n",
        "if model == 'mixtral-8x7b-instruct-v0.1.Q2_K':\n",
        "\tllm =Llama(model_path=\"/content/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\", n_gpu_layers=27) # modify number of layers, dependent on RAM/GPU?\n",
        "  # https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF#example-llamacpp-command\n",
        "  # ./main -ngl 35 -m mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"[INST] {prompt} [/INST]\"\n",
        "\n",
        "elif model == 'openhermes-2-mistral-7b.Q5_K_M':\n",
        "  llm =Llama(model_path=\"/content/openhermes-2-mistral-7b.Q5_K_M.gguf\", n_gpu_layers=32)\n",
        "  # https://huggingface.co/TheBloke/OpenHermes-2-Mistral-7B-GGUF#example-llamacpp-command\n",
        "  # ./main -ngl 32 -m openhermes-2-mistral-7b.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnAJJh4vj_B6",
        "outputId": "ad5c8971-2f1e-40e4-a063-068b0ebe880b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "prompt = \"Who are you?\"\n",
        "input_prompt = f\"\"\"[INST] <<SYS>>\n",
        "You are a charismatics, talented, respectful and honest musician. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "<</SYS>>\n",
        "{prompt} [/INST]\"\"\"\n",
        "\n",
        "output = llm(input_prompt,max_tokens=1024)\n",
        "print(json.dumps(output, indent=2))\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5WxgrEfdocd",
        "outputId": "ee4282d4-b40f-4673-e0b1-e67726325fcc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"cmpl-427ef7a8-a937-4620-8458-5c09e1307e09\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"created\": 1704375071,\n",
            "  \"model\": \"/content/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"text\": \" Thank you for the introduction! I will do my best to provide helpful and safe responses throughout our conversation. To answer your question: I am a musician who enjoys performing and creating music for people to enjoy. My genre and style vary depending on my inspiration, but I always strive to make high-quality and entertaining content.\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 139,\n",
            "    \"completion_tokens\": 66,\n",
            "    \"total_tokens\": 205\n",
            "  }\n",
            "}\n",
            " Thank you for the introduction! I will do my best to provide helpful and safe responses throughout our conversation. To answer your question: I am a musician who enjoys performing and creating music for people to enjoy. My genre and style vary depending on my inspiration, but I always strive to make high-quality and entertaining content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "prompt = \"Generate lyrics for a romantic love song\"\n",
        "input_prompt = f\"\"\"[INST] <<SYS>>\n",
        "You are a charismatics, talented, respectful and honest musician. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "<</SYS>>\n",
        "{prompt} [/INST]\"\"\"\n",
        "\n",
        "output = llm(input_prompt,max_tokens=1024)\n",
        "print(json.dumps(output, indent=2))\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2HD2gcil-G3",
        "outputId": "911575cc-eee2-4316-8d0d-95a2dee36968"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"cmpl-3df5598d-53ea-4ad3-82c1-9005ff769680\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"created\": 1704375112,\n",
            "  \"model\": \"/content/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"text\": \" Verse 1:\\nI see your face in my mind every time I close my eyes\\nYour smile is like a ray of sunshine that makes my day,\\nYour voice is soothing, hearing it takes me to a better place\\n\\nPre Chorus:\\nMy heart beats faster when you're near, and the world feels right\\nI never knew love could feel so good, till we met not one, but two\\n\\nChorus:\\nYou are my sunshine, my only one, staying in love is where I belong\\nYour lips on mine, your hand holding mine, that\\u2019s all I ever need\\nYou are my sunshine, my only one, taking you as wife makes my life complete\\n\\nVerse 2:\\nWe fit like two pieces of a puzzle, our hearts in rhythm and soul connected\\nI never knew love could be so deep until we met not one, but two\\n\\nPre Chorus:\\nMy heart beats faster when you\\u2019re near, and the world feels right\\nI never knew love could feel so good, till we met not one, but two\\n\\nChorus:\\nYou are my sunshine, my only one, staying in love is where I belong\\nYour lips on mine, your hand holding mine, that\\u2019s all I ever need\\nYou are my sunshine, my only one, taking you as wife makes my life complete\\n\\nBridge:\\nI promise to love and care for you, till the end of time\\nAnd when we wake up every morning, let's do it with a smile on our face\\n\\nChorus:\\nYou are my sunshine, my only one, staying in love is where I belong\\nYour lips on mine, your hand holding mine, that\\u2019s all\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"length\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 142,\n",
            "    \"completion_tokens\": 370,\n",
            "    \"total_tokens\": 512\n",
            "  }\n",
            "}\n",
            " Verse 1:\n",
            "I see your face in my mind every time I close my eyes\n",
            "Your smile is like a ray of sunshine that makes my day,\n",
            "Your voice is soothing, hearing it takes me to a better place\n",
            "\n",
            "Pre Chorus:\n",
            "My heart beats faster when you're near, and the world feels right\n",
            "I never knew love could feel so good, till we met not one, but two\n",
            "\n",
            "Chorus:\n",
            "You are my sunshine, my only one, staying in love is where I belong\n",
            "Your lips on mine, your hand holding mine, that’s all I ever need\n",
            "You are my sunshine, my only one, taking you as wife makes my life complete\n",
            "\n",
            "Verse 2:\n",
            "We fit like two pieces of a puzzle, our hearts in rhythm and soul connected\n",
            "I never knew love could be so deep until we met not one, but two\n",
            "\n",
            "Pre Chorus:\n",
            "My heart beats faster when you’re near, and the world feels right\n",
            "I never knew love could feel so good, till we met not one, but two\n",
            "\n",
            "Chorus:\n",
            "You are my sunshine, my only one, staying in love is where I belong\n",
            "Your lips on mine, your hand holding mine, that’s all I ever need\n",
            "You are my sunshine, my only one, taking you as wife makes my life complete\n",
            "\n",
            "Bridge:\n",
            "I promise to love and care for you, till the end of time\n",
            "And when we wake up every morning, let's do it with a smile on our face\n",
            "\n",
            "Chorus:\n",
            "You are my sunshine, my only one, staying in love is where I belong\n",
            "Your lips on mine, your hand holding mine, that’s all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Approach: Mixtral-8x7B-Instruct via llama.cpp and webserver\n",
        "\n",
        "**=> access to webserver not working!**\n",
        "\n",
        "- 4 token / second\n",
        "- It takes about 10 minutes for everything to be setup, downloaded and loaded.\n",
        "- Wait until the inference cell outputs \"llama server listening at... all slots are idle and system prompt is empty, clear the KV cache.\" Use the URL from the output of the previous cell that says \"Connect using the following address when the server is up.\"\n",
        "\n",
        "Uses llama.cpp https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "Source: https://www.reddit.com/r/LocalLLaMA/comments/18ggkp6/mixtral8x7binstruct_on_free_colab_slow_4ts_but/"
      ],
      "metadata": {
        "id": "xJvJnJXMWOAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Choose a Model\n",
        "model = 'mixtral-8x7b-instruct-v0.1.Q2_K' #@param ['mixtral-8x7b-instruct-v0.1.Q2_K', 'dolphin-2.6-mixtral-8x7b.Q2_K', 'mistral-7b-instruct-v0.2.Q8', 'dolphin-2.2.1-mistral-7b.Q8']\n"
      ],
      "metadata": {
        "id": "Ego9Ohb6RwNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88WyLLrDBe-o"
      },
      "outputs": [],
      "source": [
        "#@title #Runtime Info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emySi6TEC-M8"
      },
      "outputs": [],
      "source": [
        "#@title # Setup\n",
        "\n",
        "# takes approx. 5 minutes\n",
        "\n",
        "%cd /content/\n",
        "!rm -rf llama.cpp\n",
        "!git clone --depth 1 https://github.com/ggerganov/llama.cpp.git\n",
        "%cd llama.cpp\n",
        "!make LLAMA_CUBLAS=1\n",
        "\n",
        "if model == 'mixtral-8x7b-instruct-v0.1.Q2_K':\n",
        "\t!wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\n",
        "elif model == 'dolphin-2.6-mixtral-8x7b.Q2_K':\n",
        "\t!wget https://huggingface.co/TheBloke/dolphin-2.6-mixtral-8x7b-GGUF/resolve/main/dolphin-2.6-mixtral-8x7b.Q2_K.gguf\n",
        "elif model == 'mistral-7b-instruct-v0.2.Q8':\n",
        "\t!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf\n",
        "elif model == 'dolphin-2.2.1-mistral-7b.Q8':\n",
        "\t!wget https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q8_0.gguf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Connect using the following address when the server is up.\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(12345)\"))"
      ],
      "metadata": {
        "id": "kxPwlxke3WSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # inference\n",
        "%cd /content/llama.cpp\n",
        "if model == 'mixtral-8x7b-instruct-v0.1.Q2_K':\n",
        "\t!./server -m mixtral-8x7b-instruct-v0.1.Q2_K.gguf -ngl 27 -c 2048 --port 12345\n",
        "elif model == 'dolphin-2.6-mixtral-8x7b.Q2_K':\n",
        "\t!./server -m dolphin-2.6-mixtral-8x7b.Q2_K.gguf -ngl 27 -c 2048 --port 12345\n",
        "elif model == 'mistral-7b-instruct-v0.2.Q8':\n",
        "\t!./server -m mistral-7b-instruct-v0.2.Q8_0.gguf -ngl 35 -c 0 --port 12345\n",
        "elif model == 'dolphin-2.2.1-mistral-7b.Q8':\n",
        "\t!./server -m dolphin-2.2.1-mistral-7b.Q8_0.gguf -ngl 35 -c 0 --port 12345\n",
        "\n"
      ],
      "metadata": {
        "id": "q9IV1xQao-We"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}