{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/Local_LLMs/blob/main/Mixtral_8x7B_llama.cpp_on_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Approach: Mixtral-8x7B-Instruct via llama.cpp-python\n",
        "\n",
        "Google T4:\n",
        "- mixtral-8x7b: 4 token / sec\n",
        "- mistral-7b: 12 token /sec"
      ],
      "metadata": {
        "id": "qNPPvNJXgPP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = 'mixtral-8x7b-instruct-v0.1.Q2_K'\n",
        "#model = 'openhermes-2-mistral-7b.Q5_K_M'\n",
        "\n",
        "if model == 'mixtral-8x7b-instruct-v0.1.Q2_K':\n",
        "\t!wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\n",
        "elif model == 'openhermes-2-mistral-7b.Q5_K_M':\n",
        "\t!wget https://huggingface.co/TheBloke/OpenHermes-2-Mistral-7B-GGUF/resolve/main/openhermes-2-mistral-7b.Q5_K_M.gguf\n",
        "elif model == 'dolphin-2.6-mixtral-8x7b.Q2_K':\n",
        "\t!wget https://huggingface.co/TheBloke/dolphin-2.6-mixtral-8x7b-GGUF/resolve/main/dolphin-2.6-mixtral-8x7b.Q2_K.gguf\n",
        "elif model == 'mistral-7b-instruct-v0.2.Q8':\n",
        "\t!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf\n",
        "elif model == 'dolphin-2.2.1-mistral-7b.Q8':\n",
        "\t!wget https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q8_0.gguf"
      ],
      "metadata": {
        "id": "pMjpE7rsjXCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "VE9heykddUQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import ctypes\n",
        "\n",
        "if model == 'mixtral-8x7b-instruct-v0.1.Q2_K':\n",
        "\tllm =Llama(model_path=\"/content/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\", n_gpu_layers=27) # modify number of layers, dependent on RAM/GPU?\n",
        "  # https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF#example-llamacpp-command\n",
        "  # ./main -ngl 35 -m mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"[INST] {prompt} [/INST]\"\n",
        "\n",
        "elif model == 'openhermes-2-mistral-7b.Q5_K_M':\n",
        "  llm =Llama(model_path=\"/content/openhermes-2-mistral-7b.Q5_K_M.gguf\", n_gpu_layers=32)\n",
        "  # https://huggingface.co/TheBloke/OpenHermes-2-Mistral-7B-GGUF#example-llamacpp-command\n",
        "  # ./main -ngl 32 -m openhermes-2-mistral-7b.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\n"
      ],
      "metadata": {
        "id": "VnAJJh4vj_B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "bJN9kfbDoQ6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "prompt = \"Who are you?\"\n",
        "input_prompt = f\"\"\"[INST] <<SYS>>\n",
        "You are a charismatics, talented, respectful and honest musician. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "<</SYS>>\n",
        "{prompt} [/INST]\"\"\"\n",
        "\n",
        "output = llm(input_prompt,max_tokens=1024)\n",
        "print(json.dumps(output, indent=2))\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "x5WxgrEfdocd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "prompt = \"Generate lyrics for a romantic love song\"\n",
        "input_prompt = f\"\"\"[INST] <<SYS>>\n",
        "You are a charismatics, talented, respectful and honest musician. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "<</SYS>>\n",
        "{prompt} [/INST]\"\"\"\n",
        "\n",
        "output = llm(input_prompt,max_tokens=1024)\n",
        "print(json.dumps(output, indent=2))\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "z2HD2gcil-G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Approach: Mixtral-8x7B-Instruct via llama.cpp and webserver\n",
        "\n",
        "**=> access to webserver not working!**\n",
        "\n",
        "- 4 token / second\n",
        "- It takes about 10 minutes for everything to be setup, downloaded and loaded.\n",
        "- Wait until the inference cell outputs \"llama server listening at... all slots are idle and system prompt is empty, clear the KV cache.\" Use the URL from the output of the previous cell that says \"Connect using the following address when the server is up.\"\n",
        "\n",
        "Uses llama.cpp https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "Source: https://www.reddit.com/r/LocalLLaMA/comments/18ggkp6/mixtral8x7binstruct_on_free_colab_slow_4ts_but/"
      ],
      "metadata": {
        "id": "xJvJnJXMWOAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Choose a Model\n",
        "model = 'mixtral-8x7b-instruct-v0.1.Q2_K' #@param ['mixtral-8x7b-instruct-v0.1.Q2_K', 'dolphin-2.6-mixtral-8x7b.Q2_K', 'mistral-7b-instruct-v0.2.Q8', 'dolphin-2.2.1-mistral-7b.Q8']\n"
      ],
      "metadata": {
        "id": "Ego9Ohb6RwNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88WyLLrDBe-o"
      },
      "outputs": [],
      "source": [
        "#@title #Runtime Info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emySi6TEC-M8"
      },
      "outputs": [],
      "source": [
        "#@title # Setup\n",
        "\n",
        "# takes approx. 5 minutes\n",
        "\n",
        "%cd /content/\n",
        "!rm -rf llama.cpp\n",
        "!git clone --depth 1 https://github.com/ggerganov/llama.cpp.git\n",
        "%cd llama.cpp\n",
        "!make LLAMA_CUBLAS=1\n",
        "\n",
        "if model == 'mixtral-8x7b-instruct-v0.1.Q2_K':\n",
        "\t!wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\n",
        "elif model == 'dolphin-2.6-mixtral-8x7b.Q2_K':\n",
        "\t!wget https://huggingface.co/TheBloke/dolphin-2.6-mixtral-8x7b-GGUF/resolve/main/dolphin-2.6-mixtral-8x7b.Q2_K.gguf\n",
        "elif model == 'mistral-7b-instruct-v0.2.Q8':\n",
        "\t!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf\n",
        "elif model == 'dolphin-2.2.1-mistral-7b.Q8':\n",
        "\t!wget https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q8_0.gguf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Connect using the following address when the server is up.\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(12345)\"))"
      ],
      "metadata": {
        "id": "kxPwlxke3WSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # inference\n",
        "%cd /content/llama.cpp\n",
        "if model == 'mixtral-8x7b-instruct-v0.1.Q2_K':\n",
        "\t!./server -m mixtral-8x7b-instruct-v0.1.Q2_K.gguf -ngl 27 -c 2048 --port 12345\n",
        "elif model == 'dolphin-2.6-mixtral-8x7b.Q2_K':\n",
        "\t!./server -m dolphin-2.6-mixtral-8x7b.Q2_K.gguf -ngl 27 -c 2048 --port 12345\n",
        "elif model == 'mistral-7b-instruct-v0.2.Q8':\n",
        "\t!./server -m mistral-7b-instruct-v0.2.Q8_0.gguf -ngl 35 -c 0 --port 12345\n",
        "elif model == 'dolphin-2.2.1-mistral-7b.Q8':\n",
        "\t!./server -m dolphin-2.2.1-mistral-7b.Q8_0.gguf -ngl 35 -c 0 --port 12345\n",
        "\n"
      ],
      "metadata": {
        "id": "q9IV1xQao-We"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}