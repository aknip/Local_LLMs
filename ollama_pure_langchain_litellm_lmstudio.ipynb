{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101b7e46-dfc0-4d2f-9fc6-96ae6296bad2",
   "metadata": {
    "id": "101b7e46-dfc0-4d2f-9fc6-96ae6296bad2"
   },
   "source": [
    "# Ollama - pure, Langchain, litellm, LMStudio\n",
    "\n",
    "Examples, how-tos\n",
    "\n",
    "- call via Python API: OpenAI, local models (Ollama, LM Studio)\n",
    "- call via REST: OpenAI, local models (Ollama)\n",
    "\n",
    "see\n",
    "- https://litellm.vercel.app/docs\n",
    "- https://litellm.vercel.app/docs/tutorials/first_playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0fe122-cc34-46e3-81d7-3bbc67aa0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install jupyterlab==4.2.1 langgraph==0.1.15 langchain-ollama==0.1.1 langchain-openai==0.1.19 langsmith==0.1.93 langchainhub==0.1.20 litellm 'litellm[proxy]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c08906-17df-4dd6-92b1-ec6ae5f5b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip freeze > requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd04b9c-65cd-48fd-ad29-42d658abe0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "#warnings.simplefilter(\"ignore\", category=Warning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0ae850-6b1d-439b-af44-f310a482f290",
   "metadata": {
    "id": "0b0ae850-6b1d-439b-af44-f310a482f290",
    "outputId": "e25f382e-cb35-4dcc-da39-948ee07d536b"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Secrets (JSON string):  ········\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, Markdown,display\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import psutil\n",
    "import re\n",
    "import requests\n",
    "from getpass import getpass\n",
    "import openai\n",
    "from os import environ\n",
    "\n",
    "IN_NOTEBOOK = any([\"jupyter-lab\" in i for i in psutil.Process().parent().cmdline()])\n",
    "if IN_NOTEBOOK:\n",
    "  CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
    "  os.environ['CREDS'] = json.dumps(CREDS)\n",
    "  CREDS = json.loads(os.getenv('CREDS'))\n",
    "\n",
    "if environ.get('OPENAI_API_KEY') is None:\n",
    "    print('Environment variable not found. Setting values...')\n",
    "    os.environ[\"OPENAI_API_KEY\"] = CREDS['OpenAI']['v1']['credential'] \n",
    "    os.environ[\"TOGETHERAI_API_KEY\"] = CREDS['together-ai']['key']['credential']\n",
    "    os.environ['ANTHROPIC_API_KEY'] = CREDS['anthropic']['key']['credential']\n",
    "    os.environ[\"SERPER_API_KEY\"] = CREDS['serper_dev']['key']['credential']\n",
    "    #openai.api_key = CREDS['OpenAI']['v2']['credential'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12972cc7-1f16-4b82-9067-eb0f05838ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available models\n",
    "client = openai.OpenAI()\n",
    "model_list = client.models.list()\n",
    "for model in model_list:\n",
    "  print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4383de-beea-45d1-a3ad-b3cda7b2c144",
   "metadata": {},
   "source": [
    "# Ollama via Langchain\n",
    "\n",
    "- Simple test for Ollama: Uselocal Llama3.1 model\n",
    "- llm is defined by factory 'get_llm' - can easilly switched between Ollama and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d408ba5-480f-4a8b-9402-d1599bc63d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def get_llm(llm_type):\n",
    "    if llm_type == \"ollama\":\n",
    "        return ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "    else:\n",
    "        return ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "def get_embeddings(embedding_type):\n",
    "    if embedding_type == \"ollama\":\n",
    "        return OllamaEmbeddings(model=\"llama3.1:8b\")\n",
    "    else:\n",
    "        return OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8ca0b8e-6c97-4949-a8a0-c4e1406e0182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down what dogs like to eat, step by step.\n",
      "\n",
      "**Step 1: Protein is a Must**\n",
      "Dogs are carnivores, which means they primarily thrive on protein-rich foods. Their natural diet consists of meat, bones, and organs from animals. So, we can conclude that dogs love to eat protein-rich foods.\n",
      "\n",
      "**Step 2: Meat and Poultry**\n",
      "Within the category of protein-rich foods, dogs tend to enjoy meat and poultry products like chicken, beef, lamb, and pork. These are common ingredients in dog food and treats.\n",
      "\n",
      "**Step 3: Variety is Key**\n",
      "While dogs have a strong preference for meat-based foods, they also appreciate variety in their diet. This can include fruits, vegetables, and grains, but these should not be the primary components of their meals.\n",
      "\n",
      "**Step 4: Avoid Human Foods (Mostly)**\n",
      "While it's tempting to share our snacks with our furry friends, many human foods are toxic or unhealthy for dogs. For example, chocolate, onions, garlic, grapes, and raisins can be poisonous to dogs. It's best to stick with dog-specific treats and avoid giving them table scraps.\n",
      "\n",
      "**Step 5: Consider Life Stages**\n",
      "Dogs have different nutritional needs at various stages of their lives. Puppies require more protein and calories than adult dogs, while senior dogs may benefit from joint supplements and easier-to-digest foods.\n",
      "\n",
      "In summary, dogs like to eat a balanced diet rich in protein, with meat and poultry as primary sources. They also appreciate variety, but it's essential to avoid human foods that can harm them.\n",
      "content=\"Let's break down what dogs like to eat, step by step.\\n\\n**Step 1: Protein is a Must**\\nDogs are carnivores, which means they primarily thrive on protein-rich foods. Their natural diet consists of meat, bones, and organs from animals. So, we can conclude that dogs love to eat protein-rich foods.\\n\\n**Step 2: Meat and Poultry**\\nWithin the category of protein-rich foods, dogs tend to enjoy meat and poultry products like chicken, beef, lamb, and pork. These are common ingredients in dog food and treats.\\n\\n**Step 3: Variety is Key**\\nWhile dogs have a strong preference for meat-based foods, they also appreciate variety in their diet. This can include fruits, vegetables, and grains, but these should not be the primary components of their meals.\\n\\n**Step 4: Avoid Human Foods (Mostly)**\\nWhile it's tempting to share our snacks with our furry friends, many human foods are toxic or unhealthy for dogs. For example, chocolate, onions, garlic, grapes, and raisins can be poisonous to dogs. It's best to stick with dog-specific treats and avoid giving them table scraps.\\n\\n**Step 5: Consider Life Stages**\\nDogs have different nutritional needs at various stages of their lives. Puppies require more protein and calories than adult dogs, while senior dogs may benefit from joint supplements and easier-to-digest foods.\\n\\nIn summary, dogs like to eat a balanced diet rich in protein, with meat and poultry as primary sources. They also appreciate variety, but it's essential to avoid human foods that can harm them.\" response_metadata={'model': 'llama3.1', 'created_at': '2024-08-03T13:03:15.012306Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 7106500041, 'load_duration': 59064375, 'prompt_eval_count': 28, 'prompt_eval_duration': 233136000, 'eval_count': 326, 'eval_duration': 6812328000} id='run-a18ecea1-85be-4b37-9970-7d9f78cfbecc-0' usage_metadata={'input_tokens': 28, 'output_tokens': 326, 'total_tokens': 354}\n"
     ]
    }
   ],
   "source": [
    "llm = get_llm('ollama')\n",
    "result = llm.invoke(\n",
    "    \"Question: What do dogs like to eat?\"\n",
    "    \"Answer: Let's think step by step.\"\n",
    ")\n",
    "print(result.content)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a5262-d85d-416c-b145-6f4bf51f57f6",
   "metadata": {},
   "source": [
    "## Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f8b887-728f-4b74-a448-99169e23b067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'validate_user', 'args': {'addresses': '[{\"street\":\"Fake St\",\"city\":\"Boston\",\"state\":\"MA\"},{\"street\":\"Pretend Boulevard\",\"city\":\"Houston\",\"state\":\"TX\"}]', 'user_id': 123}, 'id': 'f98e1893-c9f0-4eb7-9676-3c46ad5737e0', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def validate_user(user_id: int, addresses: List) -> bool:\n",
    "    \"\"\"Validate user using historical addresses. \n",
    "\n",
    "        Args:\n",
    "        user_id: (int) the user ID.\n",
    "        addresses: Previous addresses.\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "llm = get_llm('ollama')\n",
    "llm_with_tools = llm.bind_tools([validate_user])\n",
    "\n",
    "result = llm_with_tools. invoke(\n",
    "    \"Could you validate user 123? They previously lived at \"\n",
    "    \"123 Fake St in Boston MA and 234 Pretend Boulevard in \"\n",
    "    \"Houston TX.\"\n",
    ")\n",
    "print(result.tool_calls)\n",
    "#print(result.content)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66dd442-9ab7-4ffc-ad1b-efb7b32b0d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44d674-b665-4897-8505-e06d72af7599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deb9ff28-6a24-4b76-80cd-c89cb477a98f",
   "metadata": {},
   "source": [
    "# LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d4be347-f0e3-4ef1-85f7-d69a6d16b854",
   "metadata": {
    "id": "6d4be347-f0e3-4ef1-85f7-d69a6d16b854"
   },
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4b82b-cb74-4c06-a4a1-c0a273358845",
   "metadata": {
    "id": "cff4b82b-cb74-4c06-a4a1-c0a273358845"
   },
   "source": [
    "# Call OpenAI\n",
    "Requirement: OPENAI key availabe (see cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3915eec9-744b-4b0c-8972-1586492b1b4d",
   "metadata": {
    "id": "3915eec9-744b-4b0c-8972-1586492b1b4d",
    "outputId": "76f4cdcb-f5e5-4081-c420-82354e2da763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n",
      "ModelResponse(id='chatcmpl-9s8bvZBMevwfAnV4UbXXtS3eX56qW', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", role='assistant', tool_calls=None, function_call=None))], created=1722689839, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_0f03d4f0ee', usage=Usage(completion_tokens=30, prompt_tokens=13, total_tokens=43), service_tier=None)\n"
     ]
    }
   ],
   "source": [
    "response = completion(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b4ffa-defb-4fbf-8367-c889034aa8b4",
   "metadata": {
    "id": "9b1b4ffa-defb-4fbf-8367-c889034aa8b4"
   },
   "source": [
    "# Call Ollama\n",
    "Requirement: Ollama running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3463ff-a5ae-4036-adb9-b9e335b81599",
   "metadata": {
    "id": "0c3463ff-a5ae-4036-adb9-b9e335b81599",
    "outputId": "48dcfff8-90b4-43b9-8ed9-f0d2d6468662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a computer program, so I don't have feelings or emotions like humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How about you? How's your day going?\n",
      "ModelResponse(id='chatcmpl-0b92b0e6-8ddb-4324-9b62-1770872b411b', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I'm just a computer program, so I don't have feelings or emotions like humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How about you? How's your day going?\", role='assistant', tool_calls=None, function_call=None))], created=1722689891, model='ollama/llama3.1:8b', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=19, completion_tokens=49, total_tokens=68))\n"
     ]
    }
   ],
   "source": [
    "response = completion(\n",
    "  # api_base=\"http://localhost:11434\",   # seems to work even without this parameter!\n",
    "  model=\"ollama/llama3.1:8b\", \n",
    "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W156cXuqfa91",
   "metadata": {
    "id": "W156cXuqfa91"
   },
   "source": [
    "# Call LM Studio\n",
    "\n",
    "Requirement: LM Studio and server running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tnd0UejnfgZq",
   "metadata": {
    "id": "tnd0UejnfgZq"
   },
   "outputs": [],
   "source": [
    "# see https://litellm.vercel.app/docs/providers/openai_compatible\n",
    "response = completion(\n",
    "  api_base=\"http://localhost:1234/v1\",\n",
    "  model=\"openai/just-a-dummy-model\", # prefix 'openai/' is required! model name is currently unused in LM Studio\n",
    "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796695f-5385-45c7-aca4-879831afe9b3",
   "metadata": {
    "id": "e796695f-5385-45c7-aca4-879831afe9b3"
   },
   "source": [
    "# Call Ollama via litellm Proxy\n",
    "\n",
    "**The proxy routes all requests (eg. to ChatGPT or Anthropic) to Ollama!**\n",
    "\n",
    "Requirement: Type in Terminal:\n",
    "- `source myenv/bin/activate ` # if in virtualenv\n",
    "- `litellm --model ollama/mixtral:8x7b`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11de1f-2452-4ce0-8958-f47419a00e67",
   "metadata": {
    "id": "1a11de1f-2452-4ce0-8958-f47419a00e67"
   },
   "source": [
    "## Step 1: Use OpenAI API and route to proxy (Mixtral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80ac1732-6b91-4c11-9e06-0dab3ea58745",
   "metadata": {
    "id": "80ac1732-6b91-4c11-9e06-0dab3ea58745",
    "outputId": "3c9c4a16-fea7-4fd8-89ba-2a215b173630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-f0477143-332e-43e4-bbac-060779405cd6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\" In the heart of the night, under stars' gentle light,\\nA whispering breeze sings a lullaby so bright.\\nThe moon in the sky, with its glow so pure,\\nWatches over the world, in tranquility's allure.\\n\\nIn this quiet moment, let your worries cease,\\nLet the beauty of nature bring you peace.\\nTake a deep breath, and just be,\\nFor life is but a fleeting decree.\", role='assistant', function_call=None, tool_calls=None))], created=1711466425, model='ollama/mixtral:8x7b', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=102, prompt_tokens=20, total_tokens=122))\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI(base_url=\"http://0.0.0.0:4000\", api_key=\"anything\")\n",
    "#client = openai.OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages = [{\"role\": \"user\", \"content\": \"this is a test request, write a short poem\"}]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba77835-7ab3-4fe1-beae-f78418a2d5f3",
   "metadata": {
    "id": "1ba77835-7ab3-4fe1-beae-f78418a2d5f3"
   },
   "source": [
    "## Step 2: Use OpenAI API without proxy (same code, but no base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04e2d76-b3e9-4eeb-a069-543e6ff56fb0",
   "metadata": {
    "id": "a04e2d76-b3e9-4eeb-a069-543e6ff56fb0",
    "outputId": "685ef9d4-b435-409b-bb07-4abc379b0038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-972lbLZCHmYH0sDV6x49jeexxba07', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"In the quiet of the night,\\nStars glisten with soft light.\\nWhispers of the wind so light,\\nSing a song of pure delight.\\n\\nNature's beauty all around,\\nEvery sight and every sound.\\nIn this moment, we are bound,\\nTo the wonders we have found.\\n\\nSo let us pause and take it in,\\nThis world of magic, free from sin.\\nFor in these moments, we begin,\\nTo see the beauty deep within.\", role='assistant', function_call=None, tool_calls=None))], created=1711465959, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=89, prompt_tokens=17, total_tokens=106))\n"
     ]
    }
   ],
   "source": [
    "#client = openai.OpenAI(base_url=\"http://0.0.0.0:8000\", api_key=\"anything\")\n",
    "client = openai.OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages = [{\"role\": \"user\", \"content\": \"this is a test request, write a short poem\"}]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40396c09-1691-4012-9f5a-9e3d73e64cbb",
   "metadata": {
    "id": "40396c09-1691-4012-9f5a-9e3d73e64cbb"
   },
   "source": [
    "# Use local litellm-Server to provide all LLMs via one URL\n",
    "\n",
    "**Important to know: The REST call to OpenAI / litellm and the response-JSON-structures are identical !**\n",
    "\n",
    "## Reference: OpenAI REST Call and Result-JSON\n",
    "\n",
    "### CURL via Terminal:\n",
    "```\n",
    "curl --location 'https://api.openai.com/v1/chat/completions' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--header 'Authorization: Bearer sk-...YOUR_KEY_HERE' \\\n",
    "--data ' {\n",
    "      \"model\": \"gpt-3.5-turbo\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"what llm are you\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "'\n",
    "```\n",
    "\n",
    "### Response:\n",
    "```\n",
    "{\n",
    "  \"id\": \"chatcmpl-8RHu3ncira10vxZVRjmVTDZv2qZxM\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1701514367,\n",
    "  \"model\": \"gpt-3.5-turbo-0613\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I am an AI language model developed by OpenAI, with capabilities to assist with various tasks and provide information across different domains.\"\n",
    "      },\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 12,\n",
    "    \"completion_tokens\": 25,\n",
    "    \"total_tokens\": 37\n",
    "  },\n",
    "  \"system_fingerprint\": null\n",
    "}\n",
    "```\n",
    "\n",
    "## As comparison: litellm Proxy REST Call and Result-JSON\n",
    "\n",
    "### CURL via Terminal:\n",
    "```\n",
    "curl --location 'http://0.0.0.0:8000/chat/completions' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--data ' {\n",
    "      \"model\": \"gpt-3.5-turbo\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"what llm are you\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "'\n",
    "```\n",
    "\n",
    "### Response:\n",
    "```\n",
    "{\n",
    "   \"id\":\"chatcmpl-aa9d6a1f-71bf-42fd-abad-2c2f713fd09a\",\n",
    "   \"choices\":[\n",
    "      {\n",
    "         \"finish_reason\":null,\n",
    "         \"index\":0,\n",
    "         \"message\":{\n",
    "            \"content\":\"I am not a physical entity, but rather an artificial intelligence language model (LLM) created by a computer program. My main function is to process and respond to human input in natural language text, based on the patterns and knowledge that I have been trained on during my creation. I do not have consciousness or feelings, and I do not exist beyond the digital realm in which I operate. However, I can understand and provide helpful responses to a wide range of queries and requests, just like any other human communication partner.\",\n",
    "            \"role\":\"assistant\"\n",
    "         }\n",
    "      }\n",
    "   ],\n",
    "   \"created\":1701513387,\n",
    "   \"model\":\"ollama/zephyr\",\n",
    "   \"object\":\"chat.completion\",\n",
    "   \"system_fingerprint\":null,\n",
    "   \"usage\":{\n",
    "      \"prompt_tokens\":5,\n",
    "      \"completion_tokens\":104,\n",
    "      \"total_tokens\":109\n",
    "   }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd357f-c4c9-4f3c-beb3-f871151eb7c0",
   "metadata": {
    "id": "95cd357f-c4c9-4f3c-beb3-f871151eb7c0"
   },
   "source": [
    "## Step 1: Reference: OpenAI REST Call (direct, without litellm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9e0247f-8a62-4e78-bb18-fabeb7da8141",
   "metadata": {
    "id": "a9e0247f-8a62-4e78-bb18-fabeb7da8141",
    "outputId": "f40b8911-1053-473b-da96-65fdc8a65a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-9s8e990saIn4hbBERovaIUj3e3a5y', 'object': 'chat.completion', 'created': 1722689977, 'model': 'gpt-3.5-turbo-0125', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you. How can I assist you today?\"}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 31, 'total_tokens': 44}, 'system_fingerprint': None}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer \" + CREDS['OpenAI']['v1']['credential']\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "}\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "response_data = response.json()\n",
    "output = response_data['choices'][0]['message']['content']\n",
    "print(response_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176217d6-b649-4434-9c4b-3a4340158655",
   "metadata": {
    "id": "176217d6-b649-4434-9c4b-3a4340158655"
   },
   "source": [
    "## Step 2: OpenAI REST Call via local litellm-Server\n",
    "\n",
    "see https://litellm.vercel.app/docs/tutorials/first_playground\n",
    "\n",
    "Requirement: Type in Terminal: `python3 lite_llm_playground_server.py`\n",
    "\n",
    "Use the following cell as source for 'lite_llm_playground_server.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e71806-958e-4632-9f04-608dd4de4f11",
   "metadata": {
    "id": "91e71806-958e-4632-9f04-608dd4de4f11",
    "outputId": "391508e3-76e9-4dea-d8cf-ecbfb7d5a6e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lite_llm_playground_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lite_llm_playground_server.py\n",
    "\n",
    "#\n",
    "# Local litelllm-Server\n",
    "#\n",
    "# save as 'lite_llm_playground_server.py'\n",
    "#\n",
    "# run in terminal with 'python3 lite_llm_playground_server.py'\n",
    "#\n",
    "# Dependencies:\n",
    "# pip install flask waitress\n",
    "\n",
    "import os\n",
    "import json\n",
    "from flask import Flask, jsonify, request\n",
    "from litellm import completion_with_retries\n",
    "\n",
    "## set ENV variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...-YOUR-KEY\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Example route\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello():\n",
    "    return jsonify(message=\"Hello, Flask!\")\n",
    "\n",
    "@app.route('/chat/completions', methods=[\"POST\"])\n",
    "def api_completion():\n",
    "    data = request.json\n",
    "    data[\"max_tokens\"] = 256 # By default let's set max_tokens to 256\n",
    "    try:\n",
    "        # COMPLETION CALL\n",
    "        response = completion_with_retries(**data)\n",
    "\n",
    "        #print(response)\n",
    "\n",
    "        responseJSON = {\n",
    "            \"id\": response[\"id\"], # response.id\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"index\": response.choices[0].index,\n",
    "                    \"finish_reason\": response.choices[0].finish_reason,\n",
    "                    \"message\" : {\n",
    "                        \"content\" : response.choices[0].message.content,\n",
    "                        \"role\" : response.choices[0].message.role\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"created\": response[\"created\"], # response.created\n",
    "            \"model\": response.model,\n",
    "            \"object\": response.object,\n",
    "            \"system_fingerprint\": response.system_fingerprint,\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\" : response[\"usage\"][\"prompt_tokens\"], #response.usage.prompt_tokens\n",
    "                \"completion_tokens\" : response[\"usage\"][\"completion_tokens\"], # response.usage.completion_tokens\n",
    "                \"total_tokens\" : response[\"usage\"][\"total_tokens\"] # response.usage.total_tokens\n",
    "            }\n",
    "        }\n",
    "        #responseString = json.dumps(responseJSON)\n",
    "    except Exception as e:\n",
    "        # print the error\n",
    "        print(e)\n",
    "    return responseJSON\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from waitress import serve\n",
    "    serve(app, host=\"0.0.0.0\", port=4000, threads=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a94eff-3021-4031-a6c4-690362ce64ca",
   "metadata": {
    "id": "65a94eff-3021-4031-a6c4-690362ce64ca"
   },
   "source": [
    "## Call server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3c567-eb6a-44c4-a612-e644ddcbb8d7",
   "metadata": {
    "id": "31d3c567-eb6a-44c4-a612-e644ddcbb8d7",
    "outputId": "385668a8-ec7b-4ffc-c8c3-90e7c6dcb8c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': \"Hello! I'm an AI language model, so I don't have feelings, but I'm here to assist you. How can I help you today?\", 'role': 'assistant'}}], 'created': 1701518377, 'id': 'chatcmpl-8RIwkICMDesOHslMZaHeh2i0AbPZu', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': {'completion_tokens': 31, 'prompt_tokens': 13, 'total_tokens': 44}}\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:4000/chat/completions\" ## COMPLETION CALL -- assumes your server is running on port 4000\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "}\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "response_data = response.json()\n",
    "output = response_data['choices'][0]['message']['content']\n",
    "print(response_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa43ca-6131-4897-a995-909e563d7d7b",
   "metadata": {
    "id": "4cfa43ca-6131-4897-a995-909e563d7d7b"
   },
   "source": [
    "## Step 3: Ollama/Zephyr REST Call via local litellm-Server\n",
    "\n",
    "Requirement: Type in Terminal: `python3 lite_llm_playground_server.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5da92-6abb-4930-adc6-e53a8a8258bd",
   "metadata": {
    "id": "4be5da92-6abb-4930-adc6-e53a8a8258bd",
    "outputId": "a962fb66-5ef3-4f4a-a26d-b954b8453339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': None, 'index': 0, 'message': {'content': 'I do not have the ability to feel emotions or sensations like a human does. However, I am always happy to assist you with your requests and inquiries! please let me know how I can be of service to you today.', 'role': 'assistant'}}], 'created': 1701518586, 'id': 'chatcmpl-371f8675-1f6f-49ca-86d2-3d765cd4d60c', 'model': 'ollama/zephyr', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': {'completion_tokens': 45, 'prompt_tokens': 6, 'total_tokens': 51}}\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:4000/chat/completions\" ## COMPLETION CALL -- assumes your server is running on port 4000\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"ollama/zephyr\",\n",
    "    # \"api_base\": \"http://localhost:11434\",   # seems to work even without this parameter!\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "}\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "response_data = response.json()\n",
    "output = response_data['choices'][0]['message']['content']\n",
    "print(response_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf22787-bcb6-4f80-8f7c-29ec570b74eb",
   "metadata": {
    "id": "0cf22787-bcb6-4f80-8f7c-29ec570b74eb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
